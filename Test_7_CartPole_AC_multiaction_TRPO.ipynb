{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.environ['MyNN']\n",
    "os.sys.path.append(path)\n",
    "import MyNN\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ACAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.95, lam=0.98, actor_lr=0.001, critic_lr=0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.lam = lam # GAE disc rate\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor = self._build_model()\n",
    "        self.critic = self._build_vf()\n",
    "        self.actor.lr = actor_lr\n",
    "        self.critic.lr = critic_lr\n",
    "        self.scaler = MyNN.Scaler(state_size)\n",
    "        self.replay_buff_x = None\n",
    "        self.replay_buff_y = None\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = MyNN.MyNN(self.state_size)\n",
    "        model.add(24, 'Tanh')\n",
    "        model.add(24, 'Tanh')\n",
    "        model.add(2, 'Softmax')\n",
    "        model.compile('TRPO', 'Adam')\n",
    "        return model\n",
    "    \n",
    "    def _build_vf(self):\n",
    "        model = MyNN.MyNN(self.state_size)\n",
    "        model.add(24, 'Tanh')\n",
    "        model.add(24, 'Tanh')\n",
    "        model.add(1, 'Linear')\n",
    "        model.compile('MSE', 'Adam')\n",
    "        return model\n",
    "    \n",
    "    def value_function_fit(self, x, y):\n",
    "        if self.replay_buff_x is None:\n",
    "            x_train, y_train = x, y\n",
    "        else:\n",
    "            x_train = np.hstack([x, self.replay_buff_x])\n",
    "            y_train = np.hstack([y, self.replay_buff_y])\n",
    "        self.replay_buff_x = x\n",
    "        self.replay_buff_y = y\n",
    "        self.critic.optimize(x_train, y_train, num_epochs=10, \n",
    "                             report_cost=False, batch_size=128, \n",
    "                            lr = self.critic_lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        result = self.actor.forward(state, caching='no')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def running_reward(rewards, gamma):\n",
    "    result = []\n",
    "    run_rew = 0\n",
    "    for reward in  rewards[0,:][::-1]:\n",
    "        run_rew = run_rew*gamma + reward\n",
    "        result.append(run_rew)\n",
    "    return np.array(result[::-1]).reshape(1,-1)\n",
    "\n",
    "def encode(actions, action_size):\n",
    "    result = np.zeros((action_size, actions.shape[1]))\n",
    "    result[actions, range(actions.shape[1])] = 1\n",
    "    #result[result != 1] = -1\n",
    "    return result\n",
    "\n",
    "def add_gae(traj, gamma, lam):\n",
    "    rewards = traj['rewards']*(1-gamma)\n",
    "    values = traj['values']\n",
    "    traj['tds'] = rewards - values + np.append(traj['values'][0, 1:] * agent.gamma, 0).reshape(1,-1)\n",
    "    gae = running_reward(traj['tds'], gamma*lam)\n",
    "    return gae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_game(agent, render=False):\n",
    "    state = env.reset().reshape((agent.state_size,1))\n",
    "    unscaled_states = []\n",
    "    states = []\n",
    "    probs = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    mean, var = agent.scaler.get()\n",
    "    for t in range(499):\n",
    "        if render:\n",
    "            env.render()\n",
    "        unscaled_states.append(state)\n",
    "        scaled_state = (state-mean)/var\n",
    "        states.append(scaled_state)\n",
    "        prob = agent.act(scaled_state)\n",
    "        probs.append(prob)\n",
    "        action = np.random.choice(range(agent.action_size), p=prob[:,0])\n",
    "        actions.append(action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        state = state.reshape((agent.state_size,1))\n",
    "        if done:\n",
    "            reward = -10\n",
    "        if t==498:\n",
    "            reward = 20\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    return {'unscaled': np.hstack(unscaled_states), 'states' : np.hstack(states),\n",
    "            'probs': np.hstack(probs), 'actions': np.array(actions).reshape(1,-1),\n",
    "            'rewards': np.array(rewards).reshape(1,-1),'time' : t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_n_games(agent, n=20):\n",
    "    trajectories = []\n",
    "    for i in range(n):\n",
    "        trajectory = play_game(agent)\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train_set(agent, trajectories):\n",
    "    for traj in trajectories:\n",
    "        traj['disc_sum_rew'] = running_reward(traj['rewards']*(1-agent.gamma), agent.gamma)\n",
    "        traj['values'] = agent.critic.forward(traj['states'])\n",
    "        traj['GAE'] = add_gae(traj, agent.gamma, agent.lam)\n",
    "    X = np.hstack([t['states'] for t in trajectories])\n",
    "    Y = np.hstack([t['probs'] for t in trajectories])\n",
    "    disc_sum_rew = np.hstack([t['disc_sum_rew'] for t in trajectories])\n",
    "    #values = np.hstack([t['values'] for t in trajectories])\n",
    "    adv = np.hstack([t['GAE'] for t in trajectories])\n",
    "    adv = (adv - adv.mean())/(adv.std() + 1e-6)\n",
    "    adv = np.hstack([encode(t['actions'], agent.action_size) for t in trajectories]) * adv\n",
    "    return X, Y, adv, disc_sum_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = ACAgent(state_size, action_size, actor_lr=0.001, critic_lr=0.002)\n",
    "l=0\n",
    "DKL_targ = 0.001\n",
    "beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta remains the same\n",
      "1 (41.6, 231.04000000000002) 0.0018769556959350646 1\n",
      "Beta remains the same\n",
      "2 (35.6, 130.64) 0.0005322705802646676 1\n",
      "Beta remains the same\n",
      "3 (44.6, 497.03999999999996) 0.0016935434311108235 1\n",
      "Beta remains the same\n",
      "4 (58.0, 591.6) 0.0018767329093691899 1\n",
      "Beta is too small\n",
      "Increasing beta\n",
      "5 (52.6, 373.04) 0.004523015159457567 1.5\n",
      "Beta is too small\n",
      "Increasing beta\n",
      "6 (64.2, 537.76) 0.004658483287029868 2.25\n",
      "Beta remains the same\n",
      "7 (85.4, 1304.2400000000002) 0.0015762443096416121 2.25\n",
      "Beta remains the same\n",
      "8 (86.0, 450.4) 0.0010380896505946485 2.25\n",
      "Increasing beta\n",
      "9 (121.4, 1170.6399999999999) 0.0022752077497171434 3.375\n",
      "Decreasing beta\n",
      "10 (242.0, 5538.8) 0.00019924207944218102 2.25\n",
      "Decreasing beta\n",
      "11 (198.4, 3267.4399999999996) 0.00024000737192406856 1.5\n",
      "Decreasing beta\n",
      "12 (211.8, 3066.16) 0.00028888648804013703 1.0\n",
      "Beta remains the same\n",
      "13 (240.0, 4854.8) 0.001295696189360415 1.0\n",
      "Beta is too small\n",
      "Increasing beta\n",
      "14 (237.8, 7688.160000000002) 0.0046102644400073944 1.5\n",
      "Beta remains the same\n",
      "15 (277.8, 8000.959999999999) 0.0006210331494486 1.5\n",
      "Decreasing beta\n",
      "16 (330.4, 11202.240000000002) 0.0003835457541812415 1.0\n",
      "Beta remains the same\n",
      "17 (322.4, 11833.039999999999) 0.0011638877920035523 1.0\n",
      "Beta remains the same\n",
      "18 (308.6, 10645.04) 0.0005891994845022484 1.0\n",
      "Beta remains the same\n",
      "19 (386.2, 21151.36) 0.0008439090869749 1.0\n",
      "Beta remains the same\n",
      "20 (466.6, 3943.84) 0.000883309211194458 1.0\n",
      "Beta remains the same\n",
      "21 (458.2, 6336.16) 0.0016114378682906322 1.0\n",
      "Beta remains the same\n",
      "22 (472.6, 2580.64) 0.0018809930245797816 1.0\n",
      "Beta remains the same\n",
      "23 (433.4, 7772.639999999999) 0.0008753344807330364 1.0\n",
      "Decreasing beta\n",
      "24 (415.8, 9746.16) 0.00017713868043709867 0.6666666666666666\n",
      "25 (498.0, 0.0) 0.00017713868043709867 0.6666666666666666\n",
      "Beta remains the same\n",
      "26 (384.0, 18878.4) 0.0013087802793344794 0.6666666666666666\n",
      "Beta is too small\n",
      "Increasing beta\n",
      "27 (422.0, 7681.2) 0.004856917473345708 1.0\n",
      "Decreasing beta\n",
      "28 (479.2, 965.76) 0.00028381472435411935 0.6666666666666666\n",
      "29 (498.0, 0.0) 0.00028381472435411935 0.6666666666666666\n",
      "30 (498.0, 0.0) 0.00028381472435411935 0.6666666666666666\n",
      "Decreasing beta\n",
      "31 (454.6, 7534.24) 0.0002867294859534126 0.4444444444444444\n",
      "Decreasing beta\n",
      "32 (461.6, 2019.8400000000001) 7.915762360230814e-05 0.2962962962962963\n",
      "Beta is too small\n",
      "Increasing beta\n",
      "33 (430.8, 6780.160000000001) 0.004566401154704399 0.4444444444444444\n",
      "34 (498.0, 0.0) 0.004566401154704399 0.4444444444444444\n",
      "35 (498.0, 0.0) 0.004566401154704399 0.4444444444444444\n",
      "36 (498.0, 0.0) 0.004566401154704399 0.4444444444444444\n",
      "37 (498.0, 0.0) 0.004566401154704399 0.4444444444444444\n",
      "38 (498.0, 0.0) 0.004566401154704399 0.4444444444444444\n",
      "39 (498.0, 0.0) 0.004566401154704399 0.4444444444444444\n",
      "40 (498.0, 0.0) 0.004566401154704399 0.4444444444444444\n",
      "41 (498.0, 0.0) 0.004566401154704399 0.4444444444444444\n",
      "42 (498.0, 0.0) 0.004566401154704399 0.4444444444444444\n",
      "43 (498.0, 0.0) 0.004566401154704399 0.4444444444444444\n",
      "CPU times: user 53.2 s, sys: 17.9 s, total: 1min 11s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for time in range(1, 251):\n",
    "    trajectories = play_n_games(agent, n=5)\n",
    "    agent.scaler.update(np.hstack([t['unscaled'] for t in trajectories]))\n",
    "    reward = (np.mean([t['time'] for t in trajectories]), np.var([t['time'] for t in trajectories]))\n",
    "    X_batch, Y_batch, adv, disc_sum_rew = build_train_set(agent, trajectories)\n",
    "    agent.value_function_fit(X_batch, disc_sum_rew)\n",
    "    if reward[0] != 498:\n",
    "        for i in range(10):\n",
    "            Z = agent.actor.forward(X_batch)\n",
    "            DKL = np.sum(Y_batch*np.log(np.divide(Y_batch,Z)))/Y_batch.shape[1]\n",
    "        #        print(DKL)\n",
    "            if DKL > DKL_targ*4:\n",
    "                print('Beta is too small')\n",
    "                break\n",
    "            agent.actor.cache['A0'] = X_batch\n",
    "            agent.actor.backward(Z, Y_batch, adv, beta)\n",
    "            agent.actor.number_of_updates +=1\n",
    "            agent.actor.update_parameters()\n",
    "        if DKL > DKL_targ*2:\n",
    "            beta = np.minimum(35, beta*1.5)\n",
    "            print('Increasing beta')\n",
    "        elif DKL < DKL_targ*0.5:\n",
    "            beta = np.maximum(1/35, beta/1.5)\n",
    "            print('Decreasing beta')\n",
    "        else:\n",
    "            print('Beta remains the same')\n",
    "    print(time, reward, DKL, beta)\n",
    "    if reward[0] == 498:\n",
    "        l+=1\n",
    "    else:\n",
    "        l=0\n",
    "    if l==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490.18\n",
      "CPU times: user 15 s, sys: 45.7 ms, total: 15 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    traj = play_game(agent)\n",
    "    count += traj['time']\n",
    "print(count/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traj = play_game(agent, render=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
