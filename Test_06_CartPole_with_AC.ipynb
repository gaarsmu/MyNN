{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.environ['MyNN']\n",
    "os.sys.path.append(path)\n",
    "import MyNN\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ACAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.95,learning_rate=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.actor = self._build_model()\n",
    "        self.critic = self._build_vf()\n",
    "        self.scaler = MyNN.Scaler(state_size)\n",
    "        self.replay_buff_x = None\n",
    "        self.replay_buff_y = None\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = MyNN.MyNN(self.state_size)\n",
    "        model.add(24, 'Tanh')\n",
    "        model.add(24, 'Tanh')\n",
    "        model.add(1, 'Sigmoid')\n",
    "        model.compile('Cross entropy', 'Adam')\n",
    "        return model\n",
    "    \n",
    "    def _build_vf(self):\n",
    "        model = MyNN.MyNN(self.state_size)\n",
    "        model.add(24, 'Tanh')\n",
    "        model.add(24, 'Tanh')\n",
    "        model.add(1, 'ReLU')\n",
    "        model.compile('MSE', 'Adam')\n",
    "        return model\n",
    "    \n",
    "    def value_function_fit(self, x, y):\n",
    "        if self.replay_buff_x is None:\n",
    "            x_train, y_train = x, y\n",
    "        else:\n",
    "            x_train = np.hstack([x, self.replay_buff_x])\n",
    "            y_train = np.hstack([y, self.replay_buff_y])\n",
    "        self.replay_buff_x = x\n",
    "        self.replay_buff_y = y\n",
    "        self.critic.optimize(x_train, y_train, num_epochs=10, report_cost=False, batch_size=128)\n",
    "\n",
    "    def act(self, state):\n",
    "        result = self.actor.forward(state)[0][0]\n",
    "        return 1 if result > np.random.random() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def running_reward(rewards, gamma):\n",
    "    result = []\n",
    "    run_rew = 0\n",
    "    for reward in  rewards[0,:][::-1]:\n",
    "        run_rew = run_rew*gamma + reward\n",
    "        result.append(run_rew)\n",
    "    return np.array(result[::-1]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def play_game(agent, render=False):\n",
    "    state = env.reset().reshape((agent.state_size,1))\n",
    "    unscaled_states = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    mean, var = agent.scaler.get()\n",
    "    for t in range(499):\n",
    "        if render:\n",
    "            env.render()\n",
    "        unscaled_states.append(state)\n",
    "        scaled_state = (state-mean)/var\n",
    "        states.append(scaled_state)\n",
    "        action = agent.act(scaled_state)\n",
    "        actions.append(action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            reward = -10\n",
    "        if t==498:\n",
    "            reward = 20\n",
    "        state = state.reshape((agent.state_size,1))\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    unscaled_states = np.hstack(unscaled_states)\n",
    "    states = np.hstack(states)\n",
    "    return {'unscaled': unscaled_states, 'states' : states,\n",
    "            'actions': np.array(actions).reshape(1,-1),'rewards': np.array(rewards).reshape(1,-1),\n",
    "           'time': t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_n_games(agent, n=20):\n",
    "    trajectories = []\n",
    "    for i in range(n):\n",
    "        trajectory = play_game(agent)\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_train_set(agent, trajectories):\n",
    "    for traj in trajectories:\n",
    "        traj['disc_sum_rew'] = running_reward(traj['rewards'], agent.gamma)\n",
    "        traj['values'] = agent.actor.forward(traj['states'])\n",
    "    X = np.hstack([t['states'] for t in trajectories])\n",
    "    Y = np.hstack([t['actions'] for t in trajectories])\n",
    "    disc_sum_rew = np.hstack([t['disc_sum_rew'] for t in trajectories])\n",
    "    values = np.hstack([t['values'] for t in trajectories])\n",
    "    weights = disc_sum_rew - values\n",
    "    return X, Y, weights, disc_sum_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "agent = ACAgent(state_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 62.8\n",
      "2 45.6\n",
      "3 63.2\n",
      "4 106.0\n",
      "5 121.4\n",
      "6 214.0\n",
      "7 382.6\n",
      "8 228.8\n",
      "9 304.6\n",
      "10 323.4\n",
      "11 182.0\n",
      "12 236.4\n",
      "13 256.2\n",
      "14 235.2\n",
      "15 253.6\n",
      "16 178.0\n",
      "17 232.6\n",
      "18 176.4\n",
      "19 173.8\n",
      "20 188.0\n",
      "21 184.4\n",
      "22 248.4\n",
      "23 216.4\n",
      "24 264.6\n",
      "25 229.8\n",
      "26 168.4\n",
      "27 196.2\n",
      "28 199.2\n",
      "29 173.4\n",
      "30 222.8\n",
      "31 199.6\n",
      "32 214.0\n",
      "33 278.0\n",
      "34 386.2\n",
      "35 279.4\n",
      "36 280.4\n",
      "37 302.0\n",
      "38 358.0\n",
      "39 321.2\n",
      "40 349.6\n",
      "41 289.0\n",
      "42 389.4\n",
      "43 357.6\n",
      "44 331.0\n",
      "45 341.4\n",
      "46 398.4\n",
      "47 398.6\n",
      "48 383.2\n",
      "49 397.2\n",
      "50 411.8\n",
      "51 374.2\n",
      "52 434.2\n",
      "53 451.0\n",
      "54 417.0\n",
      "55 481.0\n",
      "56 469.8\n",
      "57 485.4\n",
      "58 488.4\n",
      "59 496.8\n",
      "60 498.0\n",
      "CPU times: user 51.9 s, sys: 12.4 s, total: 1min 4s\n",
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for time in range(1, 10001):\n",
    "    trajectories = play_n_games(agent, n=5)\n",
    "    agent.scaler.update(np.hstack([t['unscaled'] for t in trajectories]))\n",
    "    reward = np.mean([t['time'] for t in trajectories])\n",
    "    print(time, reward)\n",
    "    if reward == 498:\n",
    "        break\n",
    "    X_batch, Y_batch, weights, disc_sum_rew = build_train_set(agent, trajectories)\n",
    "    agent.actor.optimize(X_batch, Y_batch, weights=weights,\n",
    "                         lr=agent.learning_rate, num_epochs=1, report_cost=False)\n",
    "    agent.value_function_fit(X_batch, disc_sum_rew)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
